# flake8: noqa
from datetime import datetime
import itertools
import json
import multiprocessing
from dataclasses import dataclass
from inspect import signature
from pathlib import Path
from typing import (
    Dict,
    Iterable,
    List,
    Optional,
    Tuple,
    Type,
    TypedDict,
    Union,
)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow import keras
from typing_extensions import NotRequired

from .callbacks import AccuracyPerEpoch, EarlyStopping
from .config import ModelConfig
from .dataloader import dataset_kfold_iterator, dump_pickle, load_pickle
from .utils.logger import ApiLogger

logger = ApiLogger(__name__)

HyperParamValue = Union[int, float]
HyperParamsDict = Dict[str, HyperParamValue]
HyperParamsDictAll = Dict[str, Iterable[HyperParamValue]]


class TrainInput(TypedDict):
    hyper_params: HyperParamsDict
    config: ModelConfig


class TrainOutput(TypedDict):
    loss: NotRequired[List[float]]
    mae: NotRequired[List[float]]
    mape: NotRequired[List[float]]
    val_loss: NotRequired[List[float]]
    val_mse: NotRequired[List[float]]
    val_mae: NotRequired[List[float]]
    val_mape: NotRequired[List[float]]
    rmse: NotRequired[List[float]]


class PickleHistory(TypedDict):
    train_input: TrainInput
    train_output: TrainOutput


@dataclass
class Trainer:
    model_class: Type[keras.Sequential]
    model_config: ModelConfig
    model_name: Optional[str] = None
    workers: int = multiprocessing.cpu_count()
    use_multiprocessing: bool = True

    def __post_init__(self) -> None:
        self._model_name = self.model_name or str(self.model_class.__name__)

    def train(
        self, hyper_params: Optional[HyperParamsDict] = None
    ) -> Union[PickleHistory, List[PickleHistory]]:
        # Resets all state generated by Keras.
        hyper_params = hyper_params or {}
        model_config = self.model_config
        kfold_splits = model_config.kfold_splits

        if kfold_splits > 1:
            # Kfolds
            pickle_histories = []  # type: List[PickleHistory]
            for kfold_case, (x_train, y_train, x_test, y_test) in enumerate(
                dataset_kfold_iterator(
                    model_config.train_data,
                    model_config.train_label,
                    kfold_splits,
                ),
                start=1,
            ):
                logger.info(f"Kfolds: {kfold_case}/{kfold_splits}")
                pickle_histories.append(
                    self._train(
                        kfold_case=kfold_case,
                        x_train=x_train,
                        y_train=y_train,
                        validation_data=(x_test, y_test),
                        hyper_params=hyper_params,
                    )
                )
            return pickle_histories
        else:
            # Normal
            return self._train(
                x_train=model_config.train_data,
                y_train=model_config.train_label,
                hyper_params=hyper_params,
            )

    def _train(
        self,
        x_train: pd.DataFrame,
        y_train: pd.DataFrame,
        validation_data: Optional[Tuple[pd.DataFrame, pd.DataFrame]] = None,
        hyper_params: Optional[HyperParamsDict] = None,
        kfold_case: Optional[int] = None,
        val_split: Optional[float] = 0.1,
    ) -> PickleHistory:
        model_config = self.model_config
        filename = self.get_filename_without_ext(
            epochs=model_config.epochs,
            hyper_params=hyper_params,
            kfold_case=kfold_case,
        )
        if Path(filename + ".keras").exists() and Path(filename + ".pickle").exists():
            logger.critical(f"Skip training: {filename}")
            return load_pickle(filename + ".pickle")
        keras.backend.clear_session()
        model = self.create_model(hyper_params)

        pickle_history: PickleHistory = PickleHistory(
            train_input=TrainInput(
                hyper_params=hyper_params or {}, config=model_config
            ),
            train_output=TrainOutput(),
        )
        logger.info(f"Start training: {pickle_history}")
        if validation_data is None:
            x_train, x_val, y_train, y_val = train_test_split(
                x_train, y_train, test_size=val_split
            )
            validation_data = (x_val, y_val)

        hist = model.fit(
            x_train,
            y_train,
            epochs=model_config.epochs,
            verbose=0,  # type: ignore
            callbacks=self.create_callbacks(),
            batch_size=model_config.batch_size,
            validation_data=validation_data,
        )

        train_output = self.get_train_output(hist.history)
        pickle_history["train_output"] = train_output
        history_mean = {
            key: np.mean(train_output[key], axis=0) for key in train_output.keys()
        }

        filename = self.get_filename_without_ext(
            epochs=len(train_output.get("loss", 0)),
            hyper_params=hyper_params,
            kfold_case=kfold_case,
        )
        logger.info(f"End training: {json.dumps(history_mean, indent=2)}")
        model.save(filename + ".keras")
        dump_pickle(filename + ".pickle", pickle_history)
        return pickle_history

    def parallel_train(self, hyper_params: HyperParamsDict):
        return self.train(hyper_params)

    def hyper_train(
        self,
        all_hyper_params: Optional[HyperParamsDictAll] = None,
    ) -> None:
        all_hyper_params = all_hyper_params or {}
        product = tuple(itertools.product(*all_hyper_params.values()))
        logger.critical(
            f"model: {self._model_name} with {len(product)} cases"  # noqa: E501
        )
        product_hyper_params = [
            dict(zip(all_hyper_params.keys(), combined_hyper_params))
            for combined_hyper_params in product
        ]
        if self.use_multiprocessing:
            logger.critical("training with multiprocessing...")
            with multiprocessing.Pool(processes=self.workers) as pool:
                results = pool.map(self.train, product_hyper_params)
        else:
            logger.critical("training without multiprocessing...")
            results = [
                self.train(hyper_params=hyper_param)
                for hyper_param in product_hyper_params
            ]

        pickled_histories = []  # type: List[PickleHistory]
        for result in results:
            if isinstance(result, list):
                pickled_histories.extend(result)
            else:
                pickled_histories.append(result)

        dump_pickle(
            self.get_filename_without_ext(add_datetime=True) + ".pickle",
            pickled_histories,
        )

    def create_callbacks(self) -> List[keras.callbacks.Callback]:
        return [
            AccuracyPerEpoch(
                print_per_epoch=self.model_config.print_per_epoch,
            ),
            EarlyStopping(patience=self.model_config.patience),
        ]

    def create_model(
        self, hyper_params: Optional[HyperParamsDict] = None
    ) -> keras.Sequential:
        hyper_params = hyper_params or {}
        return self.model_class(
            self.model_config,
            **{
                key: value
                for key, value in hyper_params.items()
                if key in signature(self.model_class.__init__).parameters
            },
        )

    def get_filename_without_ext(
        self,
        epochs: Optional[int] = None,
        hyper_params: Optional[Dict[str, Union[int, float]]] = None,
        kfold_case: Optional[int] = None,
        add_datetime: bool = False,
    ) -> str:
        model_config = self.model_config
        model_name = self._model_name
        kfold_splits = model_config.kfold_splits

        path = Path(model_config.output_path)
        path.mkdir(exist_ok=True, parents=True)
        filename = f"{model_name}"
        if epochs is not None:
            filename += f"_E{epochs}"
        if hyper_params:
            # Sort hyper_params by key
            hyper_params = dict(
                sorted((hyper_params or {}).items(), key=lambda x: x[0])
            )
            filename += "".join(
                f"[{key.upper()}={value}]" for key, value in hyper_params.items()
            )
        if kfold_case is not None:
            filename += f"_K{kfold_case}of{kfold_splits}"
        if add_datetime:
            filename += f"_{datetime.now():%Y_%m_%d_%H%M%S}"
        return str(path / filename)

    @staticmethod
    def get_train_output(hist_history: Dict[str, List[float]]) -> TrainOutput:
        if "mse" in hist_history:
            hist_history["rmse"] = np.sqrt(hist_history["mse"]).tolist()
            hist_history.pop("mse")
        return TrainOutput(**hist_history)

        # for case, combined_hyper_params in enumerate(product, start=1):
        #     train_result = self.train(
        #         case,
        #         hyper_params=dict(zip(hyper_params, combined_hyper_params)),
        #     )
        #     pickled_histories.extend(train_result) if isinstance(
        #         train_result, list
        #     ) else pickled_histories.append(train_result)
