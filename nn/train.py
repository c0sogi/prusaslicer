from dataclasses import dataclass
import itertools
import json
import multiprocessing
from inspect import signature
from pathlib import Path
from typing import Any, Iterable, Optional, Type

import numpy as np
from tensorflow import keras

from .ann import ANN
from .callbacks import AccuracyPerEpoch
from .config import ANNConfig
from .dataloader import dataset_kfold_iterator
from .logger import ApiLogger

logger = ApiLogger(__name__)


def get_checkpoint_filename(
    model_name: str,
    model_config: ANNConfig,
    case: int,
    kfold_case: Optional[int] = None,
) -> str:
    if kfold_case is None:
        return f"{model_name}_C{case}of{model_config.number_of_cases}.keras"
    return f"./output/{model_name}_C{case}of{model_config.number_of_cases}_K{kfold_case}of{model_config.kfold_splits}.keras"  # noqa: E501


def get_result_filename(
    model_name: str, epochs: int, kfold_splits: int
) -> str:
    if kfold_splits is None:
        return f"{model_name}_E{epochs}.json"
    return f"./output/{model_name}_E{epochs}_K{kfold_splits}.json"


@dataclass
class Trainer:
    model_class: Type[keras.Sequential]
    model_config: ANNConfig
    model_name: Optional[str] = None
    workers: int = multiprocessing.cpu_count()
    use_multiprocessing: bool = True

    def __post_init__(self) -> None:
        self._model_name = self.model_name or str(self.model_class.__name__)

    def train(
        self, case: int, hyper_params: Optional[dict[str, int | float]] = None
    ) -> dict[str, Any]:
        # Resets all state generated by Keras.
        hyper_params = hyper_params or {}
        model_config = self.model_config
        keras.backend.clear_session()
        callback = AccuracyPerEpoch(
            print_per_epoch=model_config.print_per_epoch
        )
        model = self.model_class(
            model_config,
            **{
                key: value
                for key, value in hyper_params.items()
                if key in signature(self.model_class.__init__).parameters
            },
        )

        output = {"case": case, **hyper_params}
        kfold_splits = model_config.kfold_splits
        path = Path("./output")
        path.mkdir(exist_ok=True, parents=True)

        logger.info(f"Start training: {output}")
        if kfold_splits > 0:
            kfold_histories: list[dict[str, Any]] = []
            for kfold_case, (x_train, y_train, x_test, y_test) in enumerate(
                dataset_kfold_iterator(
                    model_config.train_data,
                    model_config.train_label,
                    kfold_splits,
                ),
                start=1,
            ):
                logger.info(
                    f"Kfolds: {kfold_case}/{model_config.kfold_splits}"
                )
                hist = model.fit(
                    x_train,
                    y_train,
                    epochs=model_config.epochs,
                    verbose=0,  # type: ignore
                    callbacks=[callback],
                    batch_size=model_config.batch_size,
                    use_multiprocessing=self.use_multiprocessing,
                    validation_data=(x_test, y_test),
                    workers=self.workers,
                )
                model.save(
                    get_checkpoint_filename(
                        self._model_name, model_config, case, kfold_case
                    )
                )
                kfold_histories.append(
                    {
                        "kfold_case": kfold_case,
                        **{
                            metric: hist.history[metric][-1]
                            for metric in hist.history
                        },
                    }
                )
            output = {
                **output,
                "kfold": kfold_histories,
                "kfold_mean": {
                    metric: np.mean(
                        [kfold[metric] for kfold in kfold_histories]
                    )
                    for metric in kfold_histories[0]
                },
            }
            logger.info(
                f"End training: {json.dumps(kfold_histories, indent=2)}"
            )
        else:
            hist = model.fit(
                model_config.train_data,
                model_config.train_label,
                epochs=model_config.epochs,
                verbose=0,  # type: ignore
                callbacks=[callback],
                batch_size=model_config.batch_size,
                use_multiprocessing=self.use_multiprocessing,
                workers=self.workers,
            )
            model.save(
                get_checkpoint_filename(self._model_name, model_config, case)
            )
            histories = {
                metric: hist.history[metric][-1] for metric in hist.history
            }  # type: dict[str, Any]
            if "mse" in histories:
                histories["rmse"] = np.sqrt(histories["mse"])
                histories.pop("mse")
            output.update(histories)
            logger.info(f"End training: {json.dumps(histories, indent=2)}")
        return output

    def hyper_train(
        self,
        model_class: Type[ANN],
        model_config: ANNConfig,
        model_name: Optional[str] = None,
        hyper_params: Optional[dict[str, Iterable[int | float]]] = None,
    ) -> None:
        hyper_params = hyper_params or {}
        model_name = model_name or model_class.__name__
        product = tuple(itertools.product(*hyper_params.values()))
        logger.critical(
            f"model: {model_name} with {model_config.number_of_cases} cases"
        )
        buffer = []  # type: list[dict[str, Any]]
        for case, combined_hyper_params in enumerate(product, start=1):
            buffer.append(
                self.train(
                    case,
                    hyper_params=dict(
                        zip(hyper_params, combined_hyper_params)
                    ),
                )
            )
        kfold_splits = model_config.kfold_splits
        epochs = model_config.epochs
        path = Path(get_result_filename(model_name, epochs, kfold_splits))
        path.mkdir(exist_ok=True, parents=True)
        path.write_text(json.dumps(buffer, indent=2))
