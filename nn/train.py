import itertools
import json
import multiprocessing
from dataclasses import dataclass
from inspect import signature
from pathlib import Path
from typing import (
    Dict,
    Iterable,
    List,
    Optional,
    Tuple,
    Type,
    TypedDict,
    Union,
)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow import keras

from .callbacks import AccuracyPerEpoch, EarlyStopping
from .config import ANNConfig
from .dataloader import dataset_kfold_iterator, dump_pickle, load_pickle
from .logger import ApiLogger

logger = ApiLogger(__name__)


class TrainInput(TypedDict):
    case: int
    hyper_params: Dict[str, Union[int, float]]
    config: ANNConfig


class PickleHistory(TypedDict):
    train_input: TrainInput
    train_output: Union[List, Dict]


def process_history(
    hist_history: Dict[str, List[float]]
) -> Dict[str, List[float]]:
    if "mse" in hist_history:
        hist_history["rmse"] = np.sqrt(hist_history["mse"]).tolist()
        hist_history.pop("mse")
    return hist_history


@dataclass
class Trainer:
    model_class: Type[keras.Sequential]
    model_config: ANNConfig
    model_name: Optional[str] = None
    workers: int = multiprocessing.cpu_count()
    use_multiprocessing: bool = True

    def __post_init__(self) -> None:
        self._model_name = self.model_name or str(self.model_class.__name__)

    def train(
        self,
        case: int,
        hyper_params: Optional[Dict[str, Union[int, float]]] = None,
    ) -> Union[PickleHistory, List[PickleHistory]]:
        # Resets all state generated by Keras.
        hyper_params = hyper_params or {}
        model_config = self.model_config
        kfold_splits = model_config.kfold_splits

        if kfold_splits > 1:
            # Kfolds
            pickle_histories = []  # type: List[PickleHistory]
            for kfold_case, (x_train, y_train, x_test, y_test) in enumerate(
                dataset_kfold_iterator(
                    model_config.train_data,
                    model_config.train_label,
                    kfold_splits,
                ),
                start=1,
            ):
                logger.info(
                    f"Kfolds: {kfold_case}/{model_config.kfold_splits}"
                )
                pickle_histories.append(
                    self._train(
                        case=case,
                        kfold_case=kfold_case,
                        x_train=x_train,
                        y_train=y_train,
                        validation_data=(x_test, y_test),
                        hyper_params=hyper_params,
                    )
                )
            return pickle_histories
        else:
            # Normal
            return self._train(
                case=case,
                x_train=model_config.train_data,
                y_train=model_config.train_label,
                hyper_params=hyper_params,
            )

    def _train(
        self,
        case: int,
        x_train: pd.DataFrame,
        y_train: pd.DataFrame,
        validation_data: Optional[Tuple[pd.DataFrame, pd.DataFrame]] = None,
        hyper_params: Optional[Dict[str, Union[int, float]]] = None,
        kfold_case: Optional[int] = None,
        val_split: Optional[float] = 0.1,
    ) -> PickleHistory:
        filename = self.get_filename_without_ext(case, kfold_case)
        if (
            Path(filename + ".keras").exists()
            and Path(filename + ".pickle").exists()
        ):
            logger.critical(f"Skip training: {filename}")
            return load_pickle(filename + ".pickle")
        keras.backend.clear_session()
        model = self.create_model(hyper_params)
        model_config = self.model_config

        pickle_history: PickleHistory = PickleHistory(
            train_input=TrainInput(
                case=case, hyper_params=hyper_params or {}, config=model_config
            ),
            train_output={},
        )
        logger.info(f"Start training: {pickle_history}")
        if validation_data is None:
            x_train, x_val, y_train, y_val = train_test_split(
                x_train, y_train, test_size=val_split
            )
            validation_data = (x_val, y_val)

        hist = model.fit(
            x_train,
            y_train,
            epochs=model_config.epochs,
            verbose=0,  # type: ignore
            callbacks=self.create_callbacks(),
            batch_size=model_config.batch_size,
            use_multiprocessing=self.use_multiprocessing,
            workers=self.workers,
            validation_data=validation_data,
        )

        history = process_history(hist.history)
        pickle_history["train_output"] = history
        history_mean = {
            key: np.mean(history[key], axis=0) for key in history.keys()
        }

        logger.info(f"End training: {json.dumps(history_mean, indent=2)}")
        model.save(filename + ".keras")
        dump_pickle(filename + ".pickle", pickle_history)
        return pickle_history

    def hyper_train(
        self,
        hyper_params: Optional[Dict[str, Iterable[Union[int, float]]]] = None,
    ) -> None:
        hyper_params = hyper_params or {}
        product = tuple(itertools.product(*hyper_params.values()))
        logger.critical(
            f"model: {self._model_name} with {self.model_config.number_of_cases} cases"  # noqa: E501
        )
        pickled_histories = []  # type: List[PickleHistory]
        for case, combined_hyper_params in enumerate(product, start=1):
            train_result = self.train(
                case,
                hyper_params=dict(zip(hyper_params, combined_hyper_params)),
            )
            pickled_histories.extend(train_result) if isinstance(
                train_result, list
            ) else pickled_histories.append(train_result)
        dump_pickle(
            self.get_filename_without_ext() + ".pickle", pickled_histories
        )

    def create_callbacks(self) -> List[keras.callbacks.Callback]:
        return [
            AccuracyPerEpoch(
                print_per_epoch=self.model_config.print_per_epoch
            ),
            EarlyStopping(patience=self.model_config.patience),
        ]

    def create_model(
        self, hyper_params: Optional[Dict[str, Union[int, float]]] = None
    ) -> keras.Sequential:
        hyper_params = hyper_params or {}
        return self.model_class(
            self.model_config,
            **{
                key: value
                for key, value in hyper_params.items()
                if key in signature(self.model_class.__init__).parameters
            },
        )

    def get_filename_without_ext(
        self,
        case: Optional[int] = None,
        kfold_case: Optional[int] = None,
        base_dir: str = "./output",
    ) -> str:
        model_config = self.model_config
        model_name = self._model_name
        epochs = model_config.epochs
        number_of_cases = model_config.number_of_cases
        kfold_splits = model_config.kfold_splits

        path = Path(base_dir)
        path.mkdir(exist_ok=True, parents=True)
        filename = f"{model_name}_E{epochs}"
        if case is not None:
            filename += f"_C{case}of{number_of_cases}"
        if kfold_case is not None:
            filename += f"_K{kfold_case}of{kfold_splits}"
        return str(path / filename)
