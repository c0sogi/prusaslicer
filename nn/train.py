import itertools
import json
from inspect import signature
import multiprocessing
from pathlib import Path
from typing import Any, Iterable, Iterator, Optional, Tuple, Type

import numpy as np
import pandas as pd
from sklearn.model_selection import KFold
from tensorflow import keras

from .ann import ANN
from .callbacks import AccuracyPerEpoch
from .config import ANNConfig
from .logger import ApiLogger

logger = ApiLogger(__name__)


def get_checkpoint_filename(
    model_name: str,
    model_config: ANNConfig,
    case: int,
    kfold_case: Optional[int] = None,
) -> str:
    if kfold_case is None:
        return f"{model_name}_C{case}of{model_config.number_of_cases}.keras"
    return f"./output/{model_name}_C{case}of{model_config.number_of_cases}_K{kfold_case}of{model_config.kfold_splits}.keras"  # noqa: E501


def get_result_filename(
    model_name: str, epochs: int, kfold_splits: int
) -> str:
    if kfold_splits is None:
        return f"{model_name}_E{epochs}.json"
    return f"./output/{model_name}_E{epochs}_K{kfold_splits}.json"


def train(
    model_class: Type[ANN],
    model_config: ANNConfig,
    case: int,
    model_name: Optional[str] = None,
    workers: int = multiprocessing.cpu_count(),
    use_multiprocessing: bool = True,
    **kwargs: Any,
) -> dict[str, Any]:
    # Resets all state generated by Keras.
    keras.backend.clear_session()
    callback = AccuracyPerEpoch(print_per_epoch=model_config.print_per_epoch)
    model = model_class(
        model_config,
        **{
            key: value
            for key, value in kwargs.items()
            if key in signature(model_class.__init__).parameters
        },
    )

    output = {"case": case, **kwargs}
    kfold_splits = model_config.kfold_splits
    model_name = model_name or model_class.__name__
    path = Path("./output")
    path.mkdir(exist_ok=True, parents=True)

    logger.info(f"Start training: {output}")
    if kfold_splits > 0:
        kfold_histories: list[dict[str, Any]] = []
        for kfold_case, (x_train, y_train, x_test, y_test) in enumerate(
            dataset_kfold_iterator(
                model_config.train_data,
                model_config.train_label,
                kfold_splits,
            ),
            start=1,
        ):
            logger.info(f"Kfolds: {kfold_case}/{model_config.kfold_splits}")
            hist = model.fit(
                x_train,
                y_train,
                epochs=model_config.epochs,
                verbose=0,  # type: ignore
                callbacks=[callback],
                batch_size=model_config.batch_size,
                use_multiprocessing=use_multiprocessing,
                validation_data=(x_test, y_test),
                workers=workers,
            )
            model.save(
                get_checkpoint_filename(
                    model_name, model_config, case, kfold_case
                )
            )
            kfold_histories.append(
                {
                    "kfold_case": kfold_case,
                    **{
                        metric: hist.history[metric][-1]
                        for metric in hist.history
                    },
                }
            )
        output = {
            **output,
            "kfold": kfold_histories,
            "kfold_mean": {
                metric: np.mean([kfold[metric] for kfold in kfold_histories])
                for metric in kfold_histories[0]
            },
        }
        logger.info(f"End training: {json.dumps(kfold_histories, indent=2)}")
    else:
        hist = model.fit(
            model_config.train_data,
            model_config.train_label,
            epochs=model_config.epochs,
            verbose=0,  # type: ignore
            callbacks=[callback],
            batch_size=model_config.batch_size,
            use_multiprocessing=use_multiprocessing,
            workers=workers,
        )
        model.save(get_checkpoint_filename(model_name, model_config, case))
        histories = {
            metric: hist.history[metric][-1] for metric in hist.history
        }  # type: dict[str, Any]
        if "mse" in histories:
            histories["rmse"] = np.sqrt(histories["mse"])
            histories.pop("mse")
        output.update(histories)
        logger.info(f"End training: {json.dumps(histories, indent=2)}")
    return output


def hyper_train(
    model_class: Type[ANN],
    model_config: ANNConfig,
    model_name: Optional[str] = None,
    workers: int = multiprocessing.cpu_count(),
    use_multiprocessing: bool = True,
    **hyper_params: Iterable[int | float],
) -> None:
    model_name = model_name or model_class.__name__
    product = tuple(itertools.product(*hyper_params.values()))
    logger.critical(
        f"model: {model_name} with {model_config.number_of_cases} cases"
    )
    buffer = []  # type: list[dict[str, Any]]
    for case, combined_hyper_params in enumerate(product, start=1):
        buffer.append(
            train(
                model_class,
                model_config,
                case,
                model_name,
                workers,
                use_multiprocessing,
                **dict(zip(hyper_params, combined_hyper_params)),
            )
        )
    kfold_splits = model_config.kfold_splits
    epochs = model_config.epochs
    path = Path(get_result_filename(model_name, epochs, kfold_splits))
    path.mkdir(exist_ok=True, parents=True)
    path.write_text(json.dumps(buffer, indent=2))


def dataset_batch_iterator(
    x_data: pd.DataFrame, y_data: pd.DataFrame, batch_size: int = 1
) -> Iterator[Tuple[pd.DataFrame, pd.DataFrame]]:
    dataset_size = min(len(x_data), len(y_data))
    for batch_start in range(0, dataset_size, batch_size):
        batch_end = min(dataset_size, batch_start + batch_size)
        yield x_data[batch_start:batch_end], y_data[batch_start:batch_end]


def dataset_kfold_iterator(
    x_data: pd.DataFrame, y_data: pd.DataFrame, n_splits: int = 5
) -> Iterator[Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]]:
    kf = KFold(n_splits=n_splits)
    for train_index, test_index in kf.split(x_data, y_data):
        x_train, x_test = x_data.iloc[train_index], x_data.iloc[test_index]
        y_train, y_test = y_data.iloc[train_index], y_data.iloc[test_index]
        yield x_train, y_train, x_test, y_test
