{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "cols = [\"보압시간\", \"사출속도1~4\", \"보압1~2\"]\n",
    "level_col_name = \"변수 수준\"\n",
    "file_path = r\"./data/사출 실험계획표 27.xlsx\"\n",
    "\n",
    "\n",
    "excel_data = pd.read_excel(file_path)[[level_col_name] + cols]\n",
    "# # find the index of the first NaN value\n",
    "null_idx = excel_data.index[excel_data.iloc[:, 0].isna()].tolist()[0]\n",
    "\n",
    "# Display the first few rows of the dataframe to understand its structure\n",
    "level_table = excel_data.iloc[:null_idx, :].copy()\n",
    "data_table = (\n",
    "    excel_data[cols].iloc[null_idx + 2:, :].copy().reset_index(drop=True)\n",
    ")\n",
    "levels = level_table[level_col_name].tolist()\n",
    "\n",
    "# Display the first few rows of the dataframe to understand its structure\n",
    "assert data_table.shape[1] == len(\n",
    "    cols\n",
    "), \"The number of columns is not correct\"\n",
    "print(f\"Levels: {levels}\")\n",
    "print(f\"Data table shape: {data_table.shape}\")\n",
    "level_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map actual values to level values (-1, 0, 1)\n",
    "def map_to_level(value, column):\n",
    "    # Find the corresponding level for the value in the specified column\n",
    "    level = level_table[level_table[column] == value][level_col_name].values[\n",
    "        0\n",
    "    ]\n",
    "    return level\n",
    "\n",
    "\n",
    "# Initialize an empty DataFrame with the same shape as data_table\n",
    "mapped_data = pd.DataFrame(columns=cols, index=range(len(data_table)))\n",
    "\n",
    "# Map each column in data_table to its corresponding level\n",
    "for col in cols:\n",
    "    mapped_data[col] = data_table[col].apply(lambda x: map_to_level(x, col))\n",
    "\n",
    "# Convert the DataFrame to integer type\n",
    "mapped_data = mapped_data.astype(int)\n",
    "\n",
    "# Set the index name to \"Case\"\n",
    "mapped_data.index.name = \"Case\"\n",
    "\n",
    "# Set the indices to start from 1\n",
    "mapped_data.index = mapped_data.index.map(lambda x: x + 1)\n",
    "\n",
    "# Display the first few rows of the mapped data\n",
    "assert mapped_data.shape[1] == len(\n",
    "    cols\n",
    "), \"The number of columns is not correct\"\n",
    "mapped_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from nn.schemas import _read_ss_curves, group_ss_curves\n",
    "\n",
    "ss_curves = group_ss_curves(_read_ss_curves(raw_data_path=Path(\"data\")))\n",
    "ss_curves[\"Case\"] = ss_curves.index.to_series().apply(\n",
    "    lambda x: int(x.split(\"-\")[1])\n",
    ")\n",
    "ss_curves = pd.merge(\n",
    "    ss_curves.reset_index(drop=True),\n",
    "    mapped_data,\n",
    "    left_on=\"Case\",\n",
    "    right_index=True,\n",
    ").drop(columns=[\"Case\"])\n",
    "\n",
    "\n",
    "ss_curves.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from nn.inference import inference\n",
    "from nn.schemas import normalize_1d_sequence\n",
    "\n",
    "\n",
    "def pick_random_data(\n",
    "    train_inputs: np.ndarray,\n",
    "    train_outputs: np.ndarray,\n",
    "    n: int = 1,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    x_test, y_test = train_inputs, train_outputs\n",
    "    assert isinstance(x_test, np.ndarray) and isinstance(\n",
    "        y_test, np.ndarray\n",
    "    ), f\"{type(x_test)} & {type(y_test)}\"\n",
    "    assert (\n",
    "        x_test.shape[0] == y_test.shape[0]\n",
    "    ), f\"{x_test.shape} != {y_test.shape}\"\n",
    "    # pick n random data points\n",
    "    idx = random.sample(range(x_test.shape[0]), n)\n",
    "    return x_test[idx], y_test[idx]\n",
    "\n",
    "\n",
    "def make_equal_interval_data(\n",
    "    ss_curves: pd.DataFrame, seq_len: int, normalize: bool = True\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    interpolated_stresses = []\n",
    "    interpolated_strains = []\n",
    "    for _, row in ss_curves.iterrows():\n",
    "        row_strain = row[\"strain\"].astype(float)\n",
    "        row_stress = row[\"stress\"].astype(float)\n",
    "        interpolated_stresses.append(\n",
    "            normalize_1d_sequence(\n",
    "                np.column_stack((row_strain, row_stress)),\n",
    "                seq_len,\n",
    "                normalize=normalize,\n",
    "            )  # shape: (seq_len, 1)\n",
    "        )\n",
    "        if normalize:\n",
    "            interpolated_strains.append(np.linspace(0, 1, seq_len))\n",
    "        else:\n",
    "            interpolated_strains.append(\n",
    "                np.linspace(\n",
    "                    row_strain.min(), row_strain.max(), seq_len\n",
    "                ).tolist()\n",
    "            )\n",
    "    # Return equal interval strain, stress data\n",
    "    return (\n",
    "        np.array(interpolated_strains).reshape(-1, seq_len, 1),\n",
    "        np.array(interpolated_stresses).reshape(-1, seq_len, 1),\n",
    "    )\n",
    "\n",
    "\n",
    "def inference_lstm(\n",
    "    test_data: Tuple[np.ndarray, np.ndarray],\n",
    "    model_path: str,\n",
    "    tolerance: float = 0.5,\n",
    "    n: int = 5,\n",
    ") -> np.ndarray:\n",
    "    x_test, y_test = test_data\n",
    "    y_pred = inference(model_path=model_path, input_data=x_test)\n",
    "    assert y_pred.shape == y_test.shape, f\"{y_pred.shape} != {y_test.shape}\"\n",
    "    seq_len = y_pred.shape[1]\n",
    "\n",
    "    def extract_points(y: np.ndarray):\n",
    "        gap = seq_len // (n - 1)\n",
    "        last_idx = seq_len - 1\n",
    "        return tuple(y[0, min(i * gap, last_idx), 0] for i in range(n))\n",
    "\n",
    "    y_pred_points = extract_points(y_pred)[1:]\n",
    "    y_test_points = extract_points(y_test)[1:]\n",
    "    print(f\"prediction: {y_pred_points}, true: {y_test_points}\")\n",
    "    for yp, yt in zip(y_pred_points, y_test_points):\n",
    "        assert abs(yp - yt) <= yt * tolerance, f\"{yp} != {yt}\"\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def inference_ann(\n",
    "    test_data: Tuple[np.ndarray, np.ndarray],\n",
    "    model_path: str,\n",
    "    tolerance: float = 0.5,\n",
    ") -> np.ndarray:\n",
    "    x_test, y_test = test_data\n",
    "    y_pred = inference(model_path, x_test)\n",
    "    print(f\"prediction: {y_pred}, true: {y_test}\")\n",
    "\n",
    "    # Check the predictions are within the tolerance\n",
    "    strength_pred = float(y_pred[0][0])\n",
    "    strength_true = float(y_test[0][0])\n",
    "    assert (\n",
    "        abs(strength_pred - strength_true) <= strength_true * tolerance\n",
    "    ), f\"{strength_pred} != {strength_true}\"\n",
    "\n",
    "    elongation_pred = float(y_pred[0][1])\n",
    "    elongation_true = float(y_test[0][1])\n",
    "    assert (\n",
    "        abs(elongation_pred - elongation_true) <= elongation_true * tolerance\n",
    "    ), f\"{elongation_pred} != {elongation_true}\"\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from typing import List\n",
    "from uuid import uuid4\n",
    "\n",
    "from nn.ann import ANN\n",
    "from nn.config import ANNModelConfig\n",
    "from nn.dataloader import DataLoader\n",
    "from nn.train import Trainer\n",
    "from nn.utils.logger import ApiLogger\n",
    "\n",
    "logger = ApiLogger(__name__)\n",
    "\n",
    "epochs = 200\n",
    "patience = 50\n",
    "batch_size = 1\n",
    "print_per_epoch = 1\n",
    "ann_hyper_params = {\n",
    "    \"n1\": [40],\n",
    "    \"n2\": [30],\n",
    "    \"n3\": [10],\n",
    "}\n",
    "\n",
    "# Creating a DataFrame\n",
    "ann_df = ss_curves.copy()\n",
    "\n",
    "\n",
    "def get_max(x: np.ndarray) -> float:\n",
    "    return np.max(x.astype(float))\n",
    "\n",
    "\n",
    "# Calculating maximum of stress and strain for each row\n",
    "ann_df[\"strength\"] = ann_df[\"stress\"].apply(get_max)\n",
    "ann_df[\"elongation\"] = ann_df[\"strain\"].apply(get_max)\n",
    "\n",
    "# Dropping the original strain and stress columns\n",
    "ann_df.drop([\"strain\", \"stress\"], axis=1, inplace=True)\n",
    "\n",
    "ann_x_data = ann_df[cols].astype(float).to_numpy()\n",
    "ann_y_data = ann_df[[\"strength\", \"elongation\"]].astype(float).to_numpy()\n",
    "\n",
    "dim_out = ann_y_data.shape[1]\n",
    "ann_model_config = ANNModelConfig(\n",
    "    output_path=f\".tmp/{uuid4().hex}\",\n",
    "    metrics=[\"mse\", \"mae\", \"mape\"],\n",
    "    kfold_splits=0,\n",
    "    print_per_epoch=print_per_epoch,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    patience=patience,\n",
    "    loss_funcs=[\"mape\" for _ in range(dim_out)],\n",
    "    loss_weights=[1 / dim_out for _ in range(dim_out)],\n",
    "    l1_reg=None,\n",
    "    l2_reg=None,\n",
    "    dropout_rate=0.0,\n",
    "    normalize_layer=False,\n",
    "    dim_out=dim_out,\n",
    "    dim_in=len(cols),\n",
    ")\n",
    "ann_data_loader = DataLoader(\n",
    "    train_inputs=ann_x_data,\n",
    "    train_outputs=ann_y_data,\n",
    "    train_input_params=cols,\n",
    "    train_output_params=[\"strength\", \"elongation\"],\n",
    ")\n",
    "ann_trainer = Trainer(\n",
    "    data_loader=ann_data_loader,\n",
    "    model_class=ANN,\n",
    "    model_name=ANN.__name__,\n",
    "    model_config=ann_model_config,\n",
    "    workers=multiprocessing.cpu_count(),\n",
    "    use_multiprocessing=False,\n",
    ")\n",
    "\n",
    "ann_x_data.shape, ann_y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import json\n",
    "\n",
    "\n",
    "num_hyper_params = reduce(\n",
    "    lambda x, y: x * len(y), ann_hyper_params.values(), 1\n",
    ")\n",
    "ann_fstem = \"\"\n",
    "\n",
    "\n",
    "for ann_fstem, phist in ann_trainer.hyper_train(ann_hyper_params):\n",
    "    num_hyper_params -= 1\n",
    "\n",
    "    json.dumps(phist[\"train_output\"], indent=4)\n",
    "\n",
    "    inference_ann(\n",
    "        model_path=ann_fstem + \".keras\",\n",
    "        test_data=pick_random_data(ann_x_data, ann_y_data, n=1),\n",
    "    )\n",
    "\n",
    "assert ann_fstem != \"\", \"fstem is empty\"\n",
    "assert num_hyper_params == 0, f\"{num_hyper_params} != 0\"\n",
    "ann_fstem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import json\n",
    "import multiprocessing\n",
    "from uuid import uuid4\n",
    "\n",
    "from nn.config import LSTMModelConfig\n",
    "from nn.dataloader import DataLoader\n",
    "from nn.lstm import EmbeddingAttentionLSTMRegressor\n",
    "from nn.train import Trainer\n",
    "\n",
    "\n",
    "epochs = 500\n",
    "patience = 50\n",
    "batch_size = 1\n",
    "print_per_epoch = 1\n",
    "seq_len = 64\n",
    "lstm_hyper_params = {\n",
    "    \"seq_len\": [seq_len],\n",
    "}\n",
    "\n",
    "\n",
    "lstm_x_data = ss_curves[cols].astype(float).to_numpy()\n",
    "_, lstm_y_data = make_equal_interval_data(ss_curves, seq_len, normalize=True)\n",
    "\n",
    "# lstm_y_data = (\n",
    "#     ss_curves[[\"strain\", \"stress\"]]\n",
    "#     .apply(lambda x: pd.Series(normalize_1d_sequence(x, seq_len)))\n",
    "#     .to_numpy()\n",
    "# )[:, :, np.newaxis]\n",
    "# decoder_inputs = np.zeros_like(lstm_y_data)\n",
    "# decoder_inputs[:, 1:, :] = lstm_y_data[:, :-1, :]  # Teacher forcing\n",
    "\n",
    "assert lstm_x_data.shape[0] == lstm_y_data.shape[0], (\n",
    "    f\"Encoder input shape {lstm_x_data.shape} and decoder output shape {lstm_y_data.shape} \"\n",
    "    f\"do not match\"\n",
    ")\n",
    "\n",
    "lstm_model_config = LSTMModelConfig(\n",
    "    output_path=f\".tmp/{uuid4().hex}\",\n",
    "    metrics=[\"mse\", \"mae\"],\n",
    "    kfold_splits=0,\n",
    "    print_per_epoch=print_per_epoch,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    patience=patience,\n",
    "    loss_funcs=[\"mse\"],\n",
    "    loss_weights=[1.0],\n",
    "    l1_reg=None,\n",
    "    l2_reg=None,\n",
    "    dropout_rate=0.0,\n",
    "    normalize_layer=False,\n",
    "    dim_out=1,\n",
    "    ann_model_path=ann_fstem + \".keras\",\n",
    "    dim_in=len(cols),\n",
    ")\n",
    "lstm_data_loader = DataLoader(\n",
    "    train_inputs=lstm_x_data,\n",
    "    train_outputs=lstm_y_data,\n",
    "    train_input_params=cols,\n",
    "    train_output_params=[\"stress\"],\n",
    ")\n",
    "lstm_trainer = Trainer(\n",
    "    data_loader=lstm_data_loader,\n",
    "    model_class=EmbeddingAttentionLSTMRegressor,\n",
    "    model_name=EmbeddingAttentionLSTMRegressor.__name__,\n",
    "    model_config=lstm_model_config,\n",
    "    workers=multiprocessing.cpu_count(),\n",
    "    use_multiprocessing=False,\n",
    ")\n",
    "lstm_x_data.shape, lstm_y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hyper_params = reduce(\n",
    "    lambda x, y: x * len(y), lstm_hyper_params.values(), 1\n",
    ")\n",
    "\n",
    "lstm_fstem = \"\"\n",
    "\n",
    "for lstm_fstem, phist in lstm_trainer.hyper_train(lstm_hyper_params):\n",
    "    num_hyper_params -= 1\n",
    "\n",
    "    json.dumps(phist[\"train_output\"], indent=4)\n",
    "\n",
    "    inference_lstm(\n",
    "        model_path=lstm_fstem + \".keras\",\n",
    "        test_data=pick_random_data(lstm_x_data, lstm_y_data, n=1),\n",
    "    )\n",
    "\n",
    "\n",
    "assert lstm_fstem != \"\", \"fstem is empty\"\n",
    "\n",
    "\n",
    "assert num_hyper_params == 0, f\"{num_hyper_params} != 0\"\n",
    "lstm_fstem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "n = 5\n",
    "seq_len = 64\n",
    "ann_fstem = (\n",
    "    \".tmp\\\\e752431ae6834f93bef7da33e7f29891\\\\ANN_E91[N1=40][N2=30][N3=10]\"\n",
    ")\n",
    "lstm_fstem = \".tmp\\\\82957e361f6846a2ba5f7d300485264a\\\\EmbeddingAttentionLSTMRegressor_E309[SEQ_LEN=64]\"\n",
    "lstm_model = load_model(lstm_fstem + \".keras\")\n",
    "ann_model = load_model(ann_fstem + \".keras\")\n",
    "assert lstm_model is not None and ann_model is not None, \"Model is None\"\n",
    "\n",
    "strain_true, stress_true = make_equal_interval_data(\n",
    "    ss_curves, seq_len, normalize=False\n",
    ")\n",
    "model_input = ss_curves[cols].astype(float).to_numpy()\n",
    "assert (\n",
    "    strain_true.shape[0] == stress_true.shape[0] == model_input.shape[0]\n",
    "), (\n",
    "    f\"Strain shape {strain_true.shape}, stress shape {stress_true.shape} and model input shape {model_input.shape} \"\n",
    "    f\"do not match\"\n",
    ")\n",
    "\n",
    "# pick n random data points\n",
    "random_indices = random.sample(range(model_input.shape[0]), n)\n",
    "\n",
    "x = model_input[random_indices]\n",
    "stress_true = stress_true[random_indices].reshape(n, seq_len, 1)\n",
    "strain_true = strain_true[random_indices].reshape(n, seq_len, 1)\n",
    "\n",
    "ann_pred = ann_model.predict(x)  # np.array of shape (n, 2)\n",
    "lstm_pred = lstm_model.predict(x)  # np.array of shape (n, seq_len, 1)\n",
    "assert ann_pred.shape == (n, 2) and lstm_pred.shape == (n, seq_len, 1)\n",
    "\n",
    "strength_pred = ann_pred[:, 0]  # type: np.ndarray\n",
    "elongation_pred = ann_pred[:, 1]  # type: np.ndarray\n",
    "normalized_stress_pred = lstm_pred  # type: np.ndarray\n",
    "stress_pred = (\n",
    "    normalized_stress_pred * strength_pred[:, np.newaxis, np.newaxis]\n",
    ")\n",
    "assert (\n",
    "    stress_pred.shape == stress_true.shape\n",
    "), f\"stress_pred shape {stress_pred.shape} != stress_true shape {stress_true.shape}\"\n",
    "\n",
    "strain_pred = np.array(\n",
    "    [np.linspace(0, max_strain, seq_len) for max_strain in elongation_pred]\n",
    ").reshape(n, seq_len, 1)\n",
    "assert (\n",
    "    strain_pred.shape == strain_true.shape\n",
    "), f\"strain_pred shape {strain_pred.shape} != strain_true shape {strain_true.shape}\"\n",
    "\n",
    "assert (\n",
    "    strain_pred.shape == strain_true.shape\n",
    "    and stress_pred.shape == stress_true.shape\n",
    "), (\n",
    "    f\"Strain shape {strain_pred.shape}, stress shape {stress_pred.shape} and true strain shape {strain_true.shape} \"\n",
    "    f\"do not match with true stress shape {stress_true.shape}\"\n",
    ")\n",
    "\n",
    "strain_pred.shape, stress_pred.shape\n",
    "elongation_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the true and predicted stress-strain curves\n",
    "for i in range(n):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plotting the true curve\n",
    "    plt.plot(\n",
    "        strain_true[i, :, 0],\n",
    "        stress_true[i, :, 0],\n",
    "        label=\"True\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "\n",
    "    # Plotting the predicted curve\n",
    "    plt.plot(\n",
    "        strain_pred[i, :, 0],\n",
    "        stress_pred[i, :, 0],\n",
    "        label=\"Predicted\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Strain\")\n",
    "    plt.ylabel(\"Stress\")\n",
    "    plt.title(f\"Stress-Strain Curve for Sample {i+1}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
