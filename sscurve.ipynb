{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levels: [-1, 0, 1]\n",
      "Data table shape: (27, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>변수 수준</th>\n",
       "      <th>보압시간</th>\n",
       "      <th>사출속도1~4</th>\n",
       "      <th>보압1~2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  변수 수준 보압시간 사출속도1~4 보압1~2\n",
       "0    -1  1.2      40    10\n",
       "1     0  1.4      50    20\n",
       "2     1  1.6      60    30"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "cols = [\"보압시간\", \"사출속도1~4\", \"보압1~2\"]\n",
    "level_col_name = \"변수 수준\"\n",
    "file_path = r\"C:\\Users\\dcas\\Documents\\카카오톡 받은 파일\\사출 실험계획표 27.xlsx\"\n",
    "\n",
    "\n",
    "epoch = 100\n",
    "patience = 100\n",
    "print_per_epoch = 1\n",
    "input_params = cols\n",
    "output_params = [\"stress\"]\n",
    "\n",
    "excel_data = pd.read_excel(file_path)[[level_col_name] + cols]\n",
    "# # find the index of the first NaN value\n",
    "null_idx = excel_data.index[excel_data.iloc[:, 0].isna()].tolist()[0]\n",
    "\n",
    "# Display the first few rows of the dataframe to understand its structure\n",
    "level_table = excel_data.iloc[:null_idx, :].copy()\n",
    "data_table = (\n",
    "    excel_data[cols].iloc[null_idx + 2 :, :].copy().reset_index(drop=True)\n",
    ")\n",
    "levels = level_table[level_col_name].tolist()\n",
    "\n",
    "# Display the first few rows of the dataframe to understand its structure\n",
    "assert data_table.shape[1] == len(\n",
    "    cols\n",
    "), \"The number of columns is not correct\"\n",
    "print(f\"Levels: {levels}\")\n",
    "print(f\"Data table shape: {data_table.shape}\")\n",
    "level_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>보압시간</th>\n",
       "      <th>사출속도1~4</th>\n",
       "      <th>보압1~2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      보압시간  사출속도1~4  보압1~2\n",
       "Case                      \n",
       "1       -1       -1     -1\n",
       "2        0       -1     -1\n",
       "3        1       -1     -1\n",
       "4       -1       -1      0\n",
       "5        0       -1      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to map actual values to level values (-1, 0, 1)\n",
    "def map_to_level(value, column):\n",
    "    # Find the corresponding level for the value in the specified column\n",
    "    level = level_table[level_table[column] == value][level_col_name].values[\n",
    "        0\n",
    "    ]\n",
    "    return level\n",
    "\n",
    "\n",
    "# Initialize an empty DataFrame with the same shape as data_table\n",
    "mapped_data = pd.DataFrame(columns=cols, index=range(len(data_table)))\n",
    "\n",
    "# Map each column in data_table to its corresponding level\n",
    "for col in cols:\n",
    "    mapped_data[col] = data_table[col].apply(lambda x: map_to_level(x, col))\n",
    "\n",
    "# Convert the DataFrame to integer type\n",
    "mapped_data = mapped_data.astype(int)\n",
    "\n",
    "# Set the index name to \"Case\"\n",
    "mapped_data.index.name = \"Case\"\n",
    "\n",
    "# Set the indices to start from 1\n",
    "mapped_data.index = mapped_data.index.map(lambda x: x + 1)\n",
    "\n",
    "# Display the first few rows of the mapped data\n",
    "assert mapped_data.shape[1] == len(\n",
    "    cols\n",
    "), \"The number of columns is not correct\"\n",
    "mapped_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((108, 3), (108, 64, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "import json\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import Tuple\n",
    "from uuid import uuid4\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nn.ann import ANN\n",
    "from nn.config import ANNModelConfig, LSTMModelConfig\n",
    "from nn.dataloader import DataLoader\n",
    "from nn.inference import inference\n",
    "from nn.lstm import EmbeddingAttentionLSTMRegressor\n",
    "from nn.schemas import (\n",
    "    normalize_1d_sequence,\n",
    "    _read_ss_curves,\n",
    "    group_ss_curves,\n",
    ")\n",
    "from nn.train import Trainer\n",
    "from nn.utils.logger import ApiLogger\n",
    "\n",
    "logger = ApiLogger(__name__)\n",
    "\n",
    "\n",
    "model_config = LSTMModelConfig(\n",
    "    output_path=f\".tmp/{uuid4().hex}\",\n",
    "    metrics=[\"mse\", \"mae\"],\n",
    "    kfold_splits=0,\n",
    "    print_per_epoch=print_per_epoch,\n",
    "    batch_size=1,\n",
    "    epochs=epoch,\n",
    "    patience=patience,\n",
    "    loss_funcs=[\"mse\"],\n",
    "    loss_weights=[1.0],\n",
    "    l1_reg=None,\n",
    "    l2_reg=None,\n",
    "    dropout_rate=0.0,\n",
    "    normalize_layer=False,\n",
    "    dim_out=1,\n",
    "    seq_len=64,\n",
    "    # ann_model_path=\"ANN_E10000[LR=0.001][N1=10][N2=10][N3=10].keras\",\n",
    ")\n",
    "ss_curves = group_ss_curves(_read_ss_curves(raw_data_path=Path(\"data\")))\n",
    "ss_curves[\"Case\"] = ss_curves.index.to_series().apply(\n",
    "    lambda x: int(x.split(\"-\")[1])\n",
    ")\n",
    "ss_curves = pd.merge(\n",
    "    ss_curves.reset_index(drop=True),\n",
    "    mapped_data,\n",
    "    left_on=\"Case\",\n",
    "    right_index=True,\n",
    ").drop(columns=[\"Case\"])\n",
    "encoder_inputs = ss_curves[cols].astype(float).to_numpy()\n",
    "decoder_outputs = (\n",
    "    ss_curves[\"stress\"]\n",
    "    .apply(\n",
    "        lambda x: pd.Series(normalize_1d_sequence(x, model_config.seq_len))\n",
    "    )\n",
    "    .to_numpy()\n",
    ")[:, :, np.newaxis]\n",
    "decoder_inputs = np.zeros_like(decoder_outputs)\n",
    "decoder_inputs[:, 1:, :] = decoder_outputs[:, :-1, :]  # Teacher forcing\n",
    "\n",
    "assert encoder_inputs.shape[0] == decoder_outputs.shape[0], (\n",
    "    f\"Encoder input shape {encoder_inputs.shape} and decoder output shape {decoder_outputs.shape} \"\n",
    "    f\"do not match\"\n",
    ")\n",
    "encoder_inputs.shape, decoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((108, 3), (108, 64, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs = encoder_inputs\n",
    "train_outputs = decoder_outputs\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    train_inputs=train_inputs,\n",
    "    train_outputs=train_outputs,\n",
    "    train_input_params=input_params,\n",
    "    train_output_params=output_params,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    data_loader=data_loader,\n",
    "    model_class=EmbeddingAttentionLSTMRegressor,\n",
    "    model_name=EmbeddingAttentionLSTMRegressor.__name__,\n",
    "    model_config=model_config,\n",
    "    workers=multiprocessing.cpu_count(),\n",
    "    use_multiprocessing=False,\n",
    ")\n",
    "train_inputs.shape, train_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1m[2023-11-23 21:26:50,230] nn.train:CRITICAL - model: EmbeddingAttentionLSTMRegressor with 1 cases\u001b[0m\n",
      "\u001b[35m\u001b[1m[2023-11-23 21:26:50,231] nn.train:CRITICAL - training without multiprocessing...\u001b[0m\n",
      "\u001b[32m[2023-11-23 21:26:50,324] nn.train:INFO - Start training: LSTMModelConfig(seed=777, print_per_epoch=1, output_path='.tmp/5c4dd40824674f728da746b0cee43fb7', metrics=['mse', 'mae'], epochs=100, batch_size=1, kfold_splits=0, patience=100, dim_in=10, dim_out=1, lr=0.001, loss_funcs=['mse'], loss_weights=[1.0], activation='relu', l1_reg=None, l2_reg=None, dropout_rate=0.0, normalize_layer=False, freeze_layers=[], seq_len=64, ann_model_path=None, state_transform_activation='tanh')\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 21:26:54,931] nn.callbacks:DEBUG - [Epoch   1  ]\trmse: 0.34850\tloss: 0.12145\tmse: 0.12145\tmae: 0.22653\tval_loss: 0.00743\tval_mse: 0.00743\tval_mae: 0.05719\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 21:26:57,264] nn.callbacks:DEBUG - [Epoch   2  ]\trmse: 0.08177\tloss: 0.00669\tmse: 0.00669\tmae: 0.04666\tval_loss: 0.00621\tval_mse: 0.00621\tval_mae: 0.04440\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 21:26:59,634] nn.callbacks:DEBUG - [Epoch   3  ]\trmse: 0.07786\tloss: 0.00606\tmse: 0.00606\tmae: 0.04061\tval_loss: 0.00626\tval_mse: 0.00626\tval_mae: 0.04487\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 21:27:01,984] nn.callbacks:DEBUG - [Epoch   4  ]\trmse: 0.07690\tloss: 0.00591\tmse: 0.00591\tmae: 0.03751\tval_loss: 0.00638\tval_mse: 0.00638\tval_mae: 0.04742\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def test_data(\n",
    "    train_inputs: np.ndarray, train_outputs: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    x_test, y_test = train_inputs, train_outputs\n",
    "    assert isinstance(x_test, np.ndarray) and isinstance(\n",
    "        y_test, np.ndarray\n",
    "    ), f\"{type(x_test)} & {type(y_test)}\"\n",
    "    assert (\n",
    "        x_test.shape[0] == y_test.shape[0]\n",
    "    ), f\"{x_test.shape} != {y_test.shape}\"\n",
    "    random_idx = random.randint(0, x_test.shape[0] - 1)\n",
    "    return (\n",
    "        x_test[random_idx : random_idx + 1],\n",
    "        y_test[random_idx : random_idx + 1],\n",
    "    )\n",
    "\n",
    "\n",
    "def test_inference(\n",
    "    test_data: Tuple[np.ndarray, np.ndarray],\n",
    "    model_path: str = r\".tmp\\172689c286e24684a2d2ba234ce454e6\\LSTM_E556[SEQ_LEN=512].keras\",\n",
    "):\n",
    "    x_test, y_test = test_data\n",
    "    y_pred = inference(model_path, x_test)\n",
    "    assert y_pred.shape == y_test.shape, f\"{y_pred.shape} != {y_test.shape}\"\n",
    "    seq_len = y_pred.shape[1]\n",
    "    n = 5\n",
    "\n",
    "    def extract_points(y: np.ndarray):\n",
    "        gap = seq_len // (n - 1)\n",
    "        last_idx = seq_len - 1\n",
    "        return tuple(y[0, min(i * gap, last_idx), 0] for i in range(n))\n",
    "\n",
    "    y_pred_points = extract_points(y_pred)[1:]\n",
    "    y_test_points = extract_points(y_test)[1:]\n",
    "    print(f\"prediction: {y_pred_points}, true: {y_test_points}\")\n",
    "    for yp, yt in zip(y_pred_points, y_test_points):\n",
    "        assert abs(yp - yt) <= yt * 0.5, f\"{yp} != {yt}\"\n",
    "\n",
    "\n",
    "all_hyper_params = {\"seq_len\": (model_config.seq_len,)}\n",
    "num_hyper_params = reduce(\n",
    "    lambda x, y: x * len(y), all_hyper_params.values(), 1\n",
    ")\n",
    "for fstem, phist in trainer.hyper_train(all_hyper_params):\n",
    "    num_hyper_params -= 1\n",
    "    json.dumps(phist[\"train_output\"], indent=4)\n",
    "    test_inference(\n",
    "        model_path=fstem + \".keras\",\n",
    "        test_data=test_data(train_inputs, train_outputs),\n",
    "    )\n",
    "assert num_hyper_params == 0, f\"{num_hyper_params} != 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
