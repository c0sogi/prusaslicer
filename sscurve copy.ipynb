{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levels: [-1, 0, 1]\n",
      "Data table shape: (27, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>변수 수준</th>\n",
       "      <th>보압시간</th>\n",
       "      <th>사출속도1~4</th>\n",
       "      <th>보압1~2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  변수 수준 보압시간 사출속도1~4 보압1~2\n",
       "0    -1  1.2      40    10\n",
       "1     0  1.4      50    20\n",
       "2     1  1.6      60    30"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "cols = [\"보압시간\", \"사출속도1~4\", \"보압1~2\"]\n",
    "level_col_name = \"변수 수준\"\n",
    "file_path = r\"./data/사출 실험계획표 27.xlsx\"\n",
    "\n",
    "\n",
    "excel_data = pd.read_excel(file_path)[[level_col_name] + cols]\n",
    "# # find the index of the first NaN value\n",
    "null_idx = excel_data.index[excel_data.iloc[:, 0].isna()].tolist()[0]\n",
    "\n",
    "# Display the first few rows of the dataframe to understand its structure\n",
    "level_table = excel_data.iloc[:null_idx, :].copy()\n",
    "data_table = (\n",
    "    excel_data[cols].iloc[null_idx + 2 :, :].copy().reset_index(drop=True)\n",
    ")\n",
    "levels = level_table[level_col_name].tolist()\n",
    "\n",
    "# Display the first few rows of the dataframe to understand its structure\n",
    "assert data_table.shape[1] == len(\n",
    "    cols\n",
    "), \"The number of columns is not correct\"\n",
    "print(f\"Levels: {levels}\")\n",
    "print(f\"Data table shape: {data_table.shape}\")\n",
    "level_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>보압시간</th>\n",
       "      <th>사출속도1~4</th>\n",
       "      <th>보압1~2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      보압시간  사출속도1~4  보압1~2\n",
       "Case                      \n",
       "1       -1       -1     -1\n",
       "2        0       -1     -1\n",
       "3        1       -1     -1\n",
       "4       -1       -1      0\n",
       "5        0       -1      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to map actual values to level values (-1, 0, 1)\n",
    "def map_to_level(value, column):\n",
    "    # Find the corresponding level for the value in the specified column\n",
    "    level = level_table[level_table[column] == value][level_col_name].values[\n",
    "        0\n",
    "    ]\n",
    "    return level\n",
    "\n",
    "\n",
    "# Initialize an empty DataFrame with the same shape as data_table\n",
    "mapped_data = pd.DataFrame(columns=cols, index=range(len(data_table)))\n",
    "\n",
    "# Map each column in data_table to its corresponding level\n",
    "for col in cols:\n",
    "    mapped_data[col] = data_table[col].apply(lambda x: map_to_level(x, col))\n",
    "\n",
    "# Convert the DataFrame to integer type\n",
    "mapped_data = mapped_data.astype(int)\n",
    "\n",
    "# Set the index name to \"Case\"\n",
    "mapped_data.index.name = \"Case\"\n",
    "\n",
    "# Set the indices to start from 1\n",
    "mapped_data.index = mapped_data.index.map(lambda x: x + 1)\n",
    "\n",
    "# Display the first few rows of the mapped data\n",
    "assert mapped_data.shape[1] == len(\n",
    "    cols\n",
    "), \"The number of columns is not correct\"\n",
    "mapped_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strain</th>\n",
       "      <th>stress</th>\n",
       "      <th>보압시간</th>\n",
       "      <th>사출속도1~4</th>\n",
       "      <th>보압1~2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.00, 0.00, 0.16, 0.20, 0.22, 0.26, 0.28, 0.3...</td>\n",
       "      <td>[0.000, 0.670, 0.760, 0.850, 0.940, 1.010, 1.1...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.00, 0.00, 8.00, 10.00, 11.00, 13.00, 14.00,...</td>\n",
       "      <td>[0.000, 0.359, 0.423, 0.487, 0.556, 0.605, 0.6...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.00, 0.00, 8.00, 10.00, 11.00, 13.00, 14.00,...</td>\n",
       "      <td>[0.000, 0.290, 0.354, 0.423, 0.497, 0.572, 0.6...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.00, 0.00, 8.00, 9.00, 11.00, 12.00, 14.00, ...</td>\n",
       "      <td>[0.000, 0.318, 0.385, 0.456, 0.513, 0.603, 0.6...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.00, 0.00, 8.00, 10.00, 11.00, 13.00, 14.00,...</td>\n",
       "      <td>[0.000, 0.369, 0.446, 0.526, 0.603, 0.664, 0.7...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              strain  \\\n",
       "0  [0.00, 0.00, 0.16, 0.20, 0.22, 0.26, 0.28, 0.3...   \n",
       "1  [0.00, 0.00, 8.00, 10.00, 11.00, 13.00, 14.00,...   \n",
       "2  [0.00, 0.00, 8.00, 10.00, 11.00, 13.00, 14.00,...   \n",
       "3  [0.00, 0.00, 8.00, 9.00, 11.00, 12.00, 14.00, ...   \n",
       "4  [0.00, 0.00, 8.00, 10.00, 11.00, 13.00, 14.00,...   \n",
       "\n",
       "                                              stress  보압시간  사출속도1~4  보압1~2  \n",
       "0  [0.000, 0.670, 0.760, 0.850, 0.940, 1.010, 1.1...    -1       -1     -1  \n",
       "1  [0.000, 0.359, 0.423, 0.487, 0.556, 0.605, 0.6...    -1       -1     -1  \n",
       "2  [0.000, 0.290, 0.354, 0.423, 0.497, 0.572, 0.6...    -1       -1     -1  \n",
       "3  [0.000, 0.318, 0.385, 0.456, 0.513, 0.603, 0.6...    -1       -1     -1  \n",
       "4  [0.000, 0.369, 0.446, 0.526, 0.603, 0.664, 0.7...    -1        0     -1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from nn.schemas import _read_ss_curves, group_ss_curves\n",
    "\n",
    "ss_curves = group_ss_curves(_read_ss_curves(raw_data_path=Path(\"data\")))\n",
    "ss_curves[\"Case\"] = ss_curves.index.to_series().apply(\n",
    "    lambda x: int(x.split(\"-\")[1])\n",
    ")\n",
    "ss_curves = pd.merge(\n",
    "    ss_curves.reset_index(drop=True),\n",
    "    mapped_data,\n",
    "    left_on=\"Case\",\n",
    "    right_index=True,\n",
    ").drop(columns=[\"Case\"])\n",
    "\n",
    "ss_curves.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from nn.inference import inference\n",
    "\n",
    "\n",
    "def pick_random_data(\n",
    "    train_inputs: np.ndarray,\n",
    "    train_outputs: np.ndarray,\n",
    "    n: int = 1,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    x_test, y_test = train_inputs, train_outputs\n",
    "    assert isinstance(x_test, np.ndarray) and isinstance(\n",
    "        y_test, np.ndarray\n",
    "    ), f\"{type(x_test)} & {type(y_test)}\"\n",
    "    assert (\n",
    "        x_test.shape[0] == y_test.shape[0]\n",
    "    ), f\"{x_test.shape} != {y_test.shape}\"\n",
    "    # pick n random data points\n",
    "    idx = random.sample(range(x_test.shape[0]), n)\n",
    "    return x_test[idx], y_test[idx]\n",
    "\n",
    "\n",
    "def inference_lstm(\n",
    "    test_data: Tuple[np.ndarray, np.ndarray],\n",
    "    model_path: str,\n",
    "    tolerance: float = 0.5,\n",
    "    n: int = 5,\n",
    "):\n",
    "    x_test, y_test = test_data\n",
    "    y_pred = inference(model_path=model_path, input_data=x_test)\n",
    "    assert y_pred.shape == y_test.shape, f\"{y_pred.shape} != {y_test.shape}\"\n",
    "    seq_len = y_pred.shape[1]\n",
    "\n",
    "    def extract_points(y: np.ndarray):\n",
    "        gap = seq_len // (n - 1)\n",
    "        last_idx = seq_len - 1\n",
    "        return tuple(y[0, min(i * gap, last_idx), 0] for i in range(n))\n",
    "\n",
    "    y_pred_points = extract_points(y_pred)[1:]\n",
    "    y_test_points = extract_points(y_test)[1:]\n",
    "    print(f\"prediction: {y_pred_points}, true: {y_test_points}\")\n",
    "    for yp, yt in zip(y_pred_points, y_test_points):\n",
    "        assert abs(yp - yt) <= yt * tolerance, f\"{yp} != {yt}\"\n",
    "\n",
    "\n",
    "def inference_ann(\n",
    "    test_data: Tuple[np.ndarray, np.ndarray],\n",
    "    model_path: str,\n",
    "    tolerance: float = 0.5,\n",
    "):\n",
    "    x_test, y_test = test_data\n",
    "    y_pred = inference(model_path, x_test)\n",
    "    print(f\"prediction: {y_pred}, true: {y_test}\")\n",
    "\n",
    "    # Check the predictions are within the tolerance\n",
    "    strength_pred = float(y_pred[0][0])\n",
    "    strength_true = float(y_test[0][0])\n",
    "    assert (\n",
    "        abs(strength_pred - strength_true) <= strength_true * tolerance\n",
    "    ), f\"{strength_pred} != {strength_true}\"\n",
    "\n",
    "    elongation_pred = float(y_pred[0][1])\n",
    "    elongation_true = float(y_test[0][1])\n",
    "    assert (\n",
    "        abs(elongation_pred - elongation_true) <= elongation_true * tolerance\n",
    "    ), f\"{elongation_pred} != {elongation_true}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((108, 3), (108, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from uuid import uuid4\n",
    "\n",
    "from nn.ann import ANN\n",
    "from nn.config import ANNModelConfig\n",
    "from nn.dataloader import DataLoader\n",
    "from nn.train import Trainer\n",
    "from nn.utils.logger import ApiLogger\n",
    "\n",
    "logger = ApiLogger(__name__)\n",
    "\n",
    "epochs = 100\n",
    "patience = 100\n",
    "batch_size = 1\n",
    "print_per_epoch = 1\n",
    "ann_hyper_params = {\n",
    "    \"n1\": [40],\n",
    "    \"n2\": [30],\n",
    "    \"n3\": [10],\n",
    "}\n",
    "\n",
    "# Creating a DataFrame\n",
    "ann_df = ss_curves.copy()\n",
    "\n",
    "# Calculating maximum of stress and strain for each row\n",
    "ann_df[\"strength\"] = ss_curves[\"stress\"].apply(max)\n",
    "ann_df[\"elongation\"] = ss_curves[\"strain\"].apply(max)\n",
    "\n",
    "# Dropping the original strain and stress columns\n",
    "ann_df.drop([\"strain\", \"stress\"], axis=1, inplace=True)\n",
    "\n",
    "ann_x_data = ann_df[cols].astype(float).to_numpy()\n",
    "ann_y_data = ann_df[[\"strength\", \"elongation\"]].astype(float).to_numpy()\n",
    "\n",
    "dim_out = ann_y_data.shape[1]\n",
    "ann_model_config = ANNModelConfig(\n",
    "    output_path=f\".tmp/{uuid4().hex}\",\n",
    "    metrics=[\"mse\", \"mae\", \"mape\"],\n",
    "    kfold_splits=0,\n",
    "    print_per_epoch=print_per_epoch,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    patience=patience,\n",
    "    loss_funcs=[\"mape\" for _ in range(dim_out)],\n",
    "    loss_weights=[1 / dim_out for _ in range(dim_out)],\n",
    "    l1_reg=None,\n",
    "    l2_reg=None,\n",
    "    dropout_rate=0.0,\n",
    "    normalize_layer=False,\n",
    "    dim_out=dim_out,\n",
    ")\n",
    "ann_data_loader = DataLoader(\n",
    "    train_inputs=ann_x_data,\n",
    "    train_outputs=ann_y_data,\n",
    "    train_input_params=cols,\n",
    "    train_output_params=[\"strength\", \"elongation\"],\n",
    ")\n",
    "ann_trainer = Trainer(\n",
    "    data_loader=ann_data_loader,\n",
    "    model_class=ANN,\n",
    "    model_name=ANN.__name__,\n",
    "    model_config=ann_model_config,\n",
    "    workers=multiprocessing.cpu_count(),\n",
    "    use_multiprocessing=False,\n",
    ")\n",
    "\n",
    "ann_x_data.shape, ann_y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1m[2023-11-23 22:15:29,494] nn.train:CRITICAL - model: ANN with 1 cases\u001b[0m\n",
      "\u001b[35m\u001b[1m[2023-11-23 22:15:29,495] nn.train:CRITICAL - training without multiprocessing...\u001b[0m\n",
      "\u001b[32m[2023-11-23 22:15:29,559] nn.train:INFO - Loading model [.tmp\\61556a412fb644f78ceba0d88e4ddab4\\ANN_E100[N1=40][N2=30][N3=10]] with last: 100\u001b[0m\n",
      "\u001b[32m[2023-11-23 22:15:29,561] nn.train:INFO - Already trained. Skipping...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "prediction: [[10.244956 99.66468 ]], true: [[ 9.995 99.   ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.tmp\\\\61556a412fb644f78ceba0d88e4ddab4\\\\ANN_E100[N1=40][N2=30][N3=10]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "import json\n",
    "\n",
    "\n",
    "num_hyper_params = reduce(\n",
    "    lambda x, y: x * len(y), ann_hyper_params.values(), 1\n",
    ")\n",
    "ann_fstem = \"\"\n",
    "\n",
    "\n",
    "for ann_fstem, phist in ann_trainer.hyper_train(ann_hyper_params):\n",
    "    num_hyper_params -= 1\n",
    "\n",
    "    json.dumps(phist[\"train_output\"], indent=4)\n",
    "\n",
    "    inference_ann(\n",
    "        model_path=ann_fstem + \".keras\",\n",
    "        test_data=pick_random_data(ann_x_data, ann_y_data, n=1),\n",
    "    )\n",
    "\n",
    "assert ann_fstem != \"\", \"fstem is empty\"\n",
    "assert num_hyper_params == 0, f\"{num_hyper_params} != 0\"\n",
    "ann_fstem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((108, 3), (108, 64, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "import json\n",
    "import multiprocessing\n",
    "from uuid import uuid4\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nn.config import LSTMModelConfig\n",
    "from nn.dataloader import DataLoader\n",
    "from nn.lstm import EmbeddingAttentionLSTMRegressor\n",
    "from nn.schemas import normalize_1d_sequence\n",
    "from nn.train import Trainer\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "patience = 100\n",
    "batch_size = 1\n",
    "print_per_epoch = 1\n",
    "seq_len = 64\n",
    "lstm_hyper_params = {\n",
    "    \"seq_len\": [seq_len],\n",
    "}\n",
    "\n",
    "\n",
    "lstm_x_data = ss_curves[cols].astype(float).to_numpy()\n",
    "lstm_y_data = (\n",
    "    ss_curves[\"stress\"]\n",
    "    .apply(lambda x: pd.Series(normalize_1d_sequence(x, seq_len)))\n",
    "    .to_numpy()\n",
    ")[:, :, np.newaxis]\n",
    "# decoder_inputs = np.zeros_like(lstm_y_data)\n",
    "# decoder_inputs[:, 1:, :] = lstm_y_data[:, :-1, :]  # Teacher forcing\n",
    "\n",
    "assert lstm_x_data.shape[0] == lstm_y_data.shape[0], (\n",
    "    f\"Encoder input shape {lstm_x_data.shape} and decoder output shape {lstm_y_data.shape} \"\n",
    "    f\"do not match\"\n",
    ")\n",
    "\n",
    "lstm_model_config = LSTMModelConfig(\n",
    "    output_path=f\".tmp/{uuid4().hex}\",\n",
    "    metrics=[\"mse\", \"mae\"],\n",
    "    kfold_splits=0,\n",
    "    print_per_epoch=print_per_epoch,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    patience=patience,\n",
    "    loss_funcs=[\"mse\"],\n",
    "    loss_weights=[1.0],\n",
    "    l1_reg=None,\n",
    "    l2_reg=None,\n",
    "    dropout_rate=0.0,\n",
    "    normalize_layer=False,\n",
    "    dim_out=1,\n",
    "    ann_model_path=ann_fstem + \".keras\",\n",
    ")\n",
    "lstm_data_loader = DataLoader(\n",
    "    train_inputs=lstm_x_data,\n",
    "    train_outputs=lstm_y_data,\n",
    "    train_input_params=cols,\n",
    "    train_output_params=[\"stress\"],\n",
    ")\n",
    "lstm_trainer = Trainer(\n",
    "    data_loader=lstm_data_loader,\n",
    "    model_class=EmbeddingAttentionLSTMRegressor,\n",
    "    model_name=EmbeddingAttentionLSTMRegressor.__name__,\n",
    "    model_config=lstm_model_config,\n",
    "    workers=multiprocessing.cpu_count(),\n",
    "    use_multiprocessing=False,\n",
    ")\n",
    "lstm_x_data.shape, lstm_y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1m[2023-11-23 22:12:03,277] nn.train:CRITICAL - model: EmbeddingAttentionLSTMRegressor with 1 cases\u001b[0m\n",
      "\u001b[35m\u001b[1m[2023-11-23 22:12:03,278] nn.train:CRITICAL - training without multiprocessing...\u001b[0m\n",
      "\u001b[32m[2023-11-23 22:12:03,316] nn.train:INFO - Start training: LSTMModelConfig(seed=777, print_per_epoch=1, output_path='.tmp/5dffab9e3202411da95ab5383ddd5240', metrics=['mse', 'mae'], epochs=100, batch_size=1, kfold_splits=0, patience=100, dim_in=10, dim_out=1, lr=0.001, loss_funcs=['mse'], loss_weights=[1.0], activation='relu', l1_reg=None, l2_reg=None, dropout_rate=0.0, normalize_layer=False, freeze_layers=[], seq_len=64, ann_model_path=None, state_transform_activation='tanh')\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:07,809] nn.callbacks:DEBUG - [Epoch   1  ]\trmse: 0.31006\tloss: 0.09614\tmse: 0.09614\tmae: 0.19529\tval_loss: 0.00645\tval_mse: 0.00645\tval_mae: 0.05052\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:10,232] nn.callbacks:DEBUG - [Epoch   2  ]\trmse: 0.08466\tloss: 0.00717\tmse: 0.00717\tmae: 0.05184\tval_loss: 0.00572\tval_mse: 0.00572\tval_mae: 0.04195\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:12,613] nn.callbacks:DEBUG - [Epoch   3  ]\trmse: 0.07980\tloss: 0.00637\tmse: 0.00637\tmae: 0.04352\tval_loss: 0.00584\tval_mse: 0.00584\tval_mae: 0.04475\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:14,998] nn.callbacks:DEBUG - [Epoch   4  ]\trmse: 0.07897\tloss: 0.00624\tmse: 0.00624\tmae: 0.04139\tval_loss: 0.00624\tval_mse: 0.00624\tval_mae: 0.04981\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:17,370] nn.callbacks:DEBUG - [Epoch   5  ]\trmse: 0.07903\tloss: 0.00625\tmse: 0.00625\tmae: 0.04104\tval_loss: 0.00613\tval_mse: 0.00613\tval_mae: 0.04903\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:19,848] nn.callbacks:DEBUG - [Epoch   6  ]\trmse: 0.07802\tloss: 0.00609\tmse: 0.00609\tmae: 0.03913\tval_loss: 0.00620\tval_mse: 0.00620\tval_mae: 0.04919\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:22,272] nn.callbacks:DEBUG - [Epoch   7  ]\trmse: 0.07806\tloss: 0.00609\tmse: 0.00609\tmae: 0.03875\tval_loss: 0.00525\tval_mse: 0.00525\tval_mae: 0.03521\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:24,700] nn.callbacks:DEBUG - [Epoch   8  ]\trmse: 0.07857\tloss: 0.00617\tmse: 0.00617\tmae: 0.03922\tval_loss: 0.00538\tval_mse: 0.00538\tval_mae: 0.03240\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:27,121] nn.callbacks:DEBUG - [Epoch   9  ]\trmse: 0.07871\tloss: 0.00619\tmse: 0.00619\tmae: 0.03886\tval_loss: 0.00518\tval_mse: 0.00518\tval_mae: 0.03022\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:29,582] nn.callbacks:DEBUG - [Epoch  10  ]\trmse: 0.07766\tloss: 0.00603\tmse: 0.00603\tmae: 0.03681\tval_loss: 0.00548\tval_mse: 0.00548\tval_mae: 0.03190\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:32,072] nn.callbacks:DEBUG - [Epoch  11  ]\trmse: 0.07871\tloss: 0.00619\tmse: 0.00619\tmae: 0.03904\tval_loss: 0.00524\tval_mse: 0.00524\tval_mae: 0.03135\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:34,561] nn.callbacks:DEBUG - [Epoch  12  ]\trmse: 0.07799\tloss: 0.00608\tmse: 0.00608\tmae: 0.03740\tval_loss: 0.00522\tval_mse: 0.00522\tval_mae: 0.03435\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:36,913] nn.callbacks:DEBUG - [Epoch  13  ]\trmse: 0.07807\tloss: 0.00609\tmse: 0.00609\tmae: 0.03774\tval_loss: 0.00531\tval_mse: 0.00531\tval_mae: 0.03528\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:39,313] nn.callbacks:DEBUG - [Epoch  14  ]\trmse: 0.08018\tloss: 0.00643\tmse: 0.00643\tmae: 0.04210\tval_loss: 0.00527\tval_mse: 0.00527\tval_mae: 0.02998\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:41,770] nn.callbacks:DEBUG - [Epoch  15  ]\trmse: 0.07898\tloss: 0.00624\tmse: 0.00624\tmae: 0.03980\tval_loss: 0.00565\tval_mse: 0.00565\tval_mae: 0.04130\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:44,193] nn.callbacks:DEBUG - [Epoch  16  ]\trmse: 0.07823\tloss: 0.00612\tmse: 0.00612\tmae: 0.03832\tval_loss: 0.00519\tval_mse: 0.00519\tval_mae: 0.03284\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:46,625] nn.callbacks:DEBUG - [Epoch  17  ]\trmse: 0.07822\tloss: 0.00612\tmse: 0.00612\tmae: 0.03816\tval_loss: 0.00558\tval_mse: 0.00558\tval_mae: 0.03131\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:49,088] nn.callbacks:DEBUG - [Epoch  18  ]\trmse: 0.07731\tloss: 0.00598\tmse: 0.00598\tmae: 0.03687\tval_loss: 0.00522\tval_mse: 0.00522\tval_mae: 0.02640\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:51,565] nn.callbacks:DEBUG - [Epoch  19  ]\trmse: 0.07885\tloss: 0.00622\tmse: 0.00622\tmae: 0.03886\tval_loss: 0.00524\tval_mse: 0.00524\tval_mae: 0.03541\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:54,033] nn.callbacks:DEBUG - [Epoch  20  ]\trmse: 0.07781\tloss: 0.00605\tmse: 0.00605\tmae: 0.03711\tval_loss: 0.00562\tval_mse: 0.00562\tval_mae: 0.04158\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:56,441] nn.callbacks:DEBUG - [Epoch  21  ]\trmse: 0.07720\tloss: 0.00596\tmse: 0.00596\tmae: 0.03596\tval_loss: 0.00576\tval_mse: 0.00576\tval_mae: 0.03069\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:12:58,831] nn.callbacks:DEBUG - [Epoch  22  ]\trmse: 0.07798\tloss: 0.00608\tmse: 0.00608\tmae: 0.03781\tval_loss: 0.00537\tval_mse: 0.00537\tval_mae: 0.03429\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:13:01,233] nn.callbacks:DEBUG - [Epoch  23  ]\trmse: 0.07834\tloss: 0.00614\tmse: 0.00614\tmae: 0.03860\tval_loss: 0.00517\tval_mse: 0.00517\tval_mae: 0.03248\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:13:03,633] nn.callbacks:DEBUG - [Epoch  24  ]\trmse: 0.07859\tloss: 0.00618\tmse: 0.00618\tmae: 0.03882\tval_loss: 0.00711\tval_mse: 0.00711\tval_mae: 0.05863\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:13:06,091] nn.callbacks:DEBUG - [Epoch  25  ]\trmse: 0.07899\tloss: 0.00624\tmse: 0.00624\tmae: 0.03901\tval_loss: 0.00565\tval_mse: 0.00565\tval_mae: 0.04236\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:13:08,509] nn.callbacks:DEBUG - [Epoch  26  ]\trmse: 0.07770\tloss: 0.00604\tmse: 0.00604\tmae: 0.03635\tval_loss: 0.00560\tval_mse: 0.00560\tval_mae: 0.04154\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:13:10,990] nn.callbacks:DEBUG - [Epoch  27  ]\trmse: 0.07699\tloss: 0.00593\tmse: 0.00593\tmae: 0.03558\tval_loss: 0.00517\tval_mse: 0.00517\tval_mae: 0.03286\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:13:13,465] nn.callbacks:DEBUG - [Epoch  28  ]\trmse: 0.07742\tloss: 0.00599\tmse: 0.00599\tmae: 0.03670\tval_loss: 0.00516\tval_mse: 0.00516\tval_mae: 0.02878\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:13:15,871] nn.callbacks:DEBUG - [Epoch  29  ]\trmse: 0.07854\tloss: 0.00617\tmse: 0.00617\tmae: 0.03820\tval_loss: 0.00519\tval_mse: 0.00519\tval_mae: 0.03341\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:13:18,259] nn.callbacks:DEBUG - [Epoch  30  ]\trmse: 0.07820\tloss: 0.00611\tmse: 0.00611\tmae: 0.03787\tval_loss: 0.00528\tval_mse: 0.00528\tval_mae: 0.03589\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:13:20,658] nn.callbacks:DEBUG - [Epoch  31  ]\trmse: 0.07641\tloss: 0.00584\tmse: 0.00584\tmae: 0.03495\tval_loss: 0.00565\tval_mse: 0.00565\tval_mae: 0.04222\u001b[0m\n",
      "\u001b[36m\u001b[2m[2023-11-23 22:13:23,100] nn.callbacks:DEBUG - [Epoch  32  ]\trmse: 0.07648\tloss: 0.00585\tmse: 0.00585\tmae: 0.03491\tval_loss: 0.00537\tval_mse: 0.00537\tval_mae: 0.03285\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\prusaslicer\\sscurve.ipynb 셀 8\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/prusaslicer/sscurve.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m num_hyper_params \u001b[39m=\u001b[39m reduce(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/prusaslicer/sscurve.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m x, y: x \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(y), lstm_hyper_params\u001b[39m.\u001b[39mvalues(), \u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/prusaslicer/sscurve.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m )\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects/prusaslicer/sscurve.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m fstem, phist \u001b[39min\u001b[39;00m lstm_trainer\u001b[39m.\u001b[39;49mhyper_train(lstm_hyper_params):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/prusaslicer/sscurve.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     num_hyper_params \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/prusaslicer/sscurve.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     json\u001b[39m.\u001b[39mdumps(phist[\u001b[39m\"\u001b[39m\u001b[39mtrain_output\u001b[39m\u001b[39m\"\u001b[39m], indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n",
      "File \u001b[1;32md:\\Projects\\prusaslicer\\nn\\train.py:208\u001b[0m, in \u001b[0;36mTrainer.hyper_train\u001b[1;34m(self, all_hyper_params)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     logger\u001b[39m.\u001b[39mcritical(\u001b[39m\"\u001b[39m\u001b[39mtraining without multiprocessing...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 208\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[0;32m    209\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(hyper_params\u001b[39m=\u001b[39;49mhyper_param)\n\u001b[0;32m    210\u001b[0m         \u001b[39mfor\u001b[39;49;00m hyper_param \u001b[39min\u001b[39;49;00m product_hyper_params\n\u001b[0;32m    211\u001b[0m     ]\n\u001b[0;32m    213\u001b[0m train_file_stems \u001b[39m=\u001b[39m []  \u001b[39m# type: List[str]\u001b[39;00m\n\u001b[0;32m    214\u001b[0m pickled_histories \u001b[39m=\u001b[39m []  \u001b[39m# type: List[PickleHistory]\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\prusaslicer\\nn\\train.py:209\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     logger\u001b[39m.\u001b[39mcritical(\u001b[39m\"\u001b[39m\u001b[39mtraining without multiprocessing...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    208\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 209\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(hyper_params\u001b[39m=\u001b[39;49mhyper_param)\n\u001b[0;32m    210\u001b[0m         \u001b[39mfor\u001b[39;00m hyper_param \u001b[39min\u001b[39;00m product_hyper_params\n\u001b[0;32m    211\u001b[0m     ]\n\u001b[0;32m    213\u001b[0m train_file_stems \u001b[39m=\u001b[39m []  \u001b[39m# type: List[str]\u001b[39;00m\n\u001b[0;32m    214\u001b[0m pickled_histories \u001b[39m=\u001b[39m []  \u001b[39m# type: List[PickleHistory]\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\prusaslicer\\nn\\train.py:94\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, hyper_params)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[39mreturn\u001b[39;00m zips\n\u001b[0;32m     92\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     \u001b[39m# Normal\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(\n\u001b[0;32m     95\u001b[0m         x_train\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_inputs,\n\u001b[0;32m     96\u001b[0m         y_train\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_outputs,\n\u001b[0;32m     97\u001b[0m         hyper_params\u001b[39m=\u001b[39;49mhyper_params,\n\u001b[0;32m     98\u001b[0m     )\n",
      "File \u001b[1;32md:\\Projects\\prusaslicer\\nn\\train.py:157\u001b[0m, in \u001b[0;36mTrainer._train\u001b[1;34m(self, x_train, y_train, validation_data, hyper_params, kfold_case)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[0;32m    149\u001b[0m     validation_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidation_data_loader \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    151\u001b[0m ):\n\u001b[0;32m    152\u001b[0m     validation_data \u001b[39m=\u001b[39m (\n\u001b[0;32m    153\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidation_data_loader\u001b[39m.\u001b[39mtrain_inputs,\n\u001b[0;32m    154\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidation_data_loader\u001b[39m.\u001b[39mtrain_outputs,\n\u001b[0;32m    155\u001b[0m     )\n\u001b[1;32m--> 157\u001b[0m hist \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    158\u001b[0m     x_train,\n\u001b[0;32m    159\u001b[0m     y_train,\n\u001b[0;32m    160\u001b[0m     epochs\u001b[39m=\u001b[39;49mmodel_config\u001b[39m.\u001b[39;49mepochs,\n\u001b[0;32m    161\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    162\u001b[0m     callbacks\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_callbacks(\n\u001b[0;32m    163\u001b[0m         start_epoch\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_current_epoch(pickle_history),\n\u001b[0;32m    164\u001b[0m         patience\u001b[39m=\u001b[39;49mmodel_config\u001b[39m.\u001b[39;49mpatience,\n\u001b[0;32m    165\u001b[0m         print_per_epoch\u001b[39m=\u001b[39;49mmodel_config\u001b[39m.\u001b[39;49mprint_per_epoch,\n\u001b[0;32m    166\u001b[0m     ),\n\u001b[0;32m    167\u001b[0m     batch_size\u001b[39m=\u001b[39;49mmodel_config\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[0;32m    168\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[0;32m    169\u001b[0m )\n\u001b[0;32m    170\u001b[0m train_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_train_output(hist\u001b[39m.\u001b[39mhistory)\n\u001b[0;32m    171\u001b[0m best_losses \u001b[39m=\u001b[39m {\n\u001b[0;32m    172\u001b[0m     key: np\u001b[39m.\u001b[39mmin(train_output[key], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    173\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m train_output\u001b[39m.\u001b[39mkeys()\n\u001b[0;32m    174\u001b[0m }\n",
      "File \u001b[1;32md:\\Projects\\prusaslicer\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Projects\\prusaslicer\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\Projects\\prusaslicer\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Projects\\prusaslicer\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\Projects\\prusaslicer\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32md:\\Projects\\prusaslicer\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32md:\\Projects\\prusaslicer\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\Projects\\prusaslicer\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32md:\\Projects\\prusaslicer\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32md:\\Projects\\prusaslicer\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_hyper_params = reduce(\n",
    "    lambda x, y: x * len(y), lstm_hyper_params.values(), 1\n",
    ")\n",
    "\n",
    "lstm_fstem = \"\"\n",
    "\n",
    "for lstm_fstem, phist in lstm_trainer.hyper_train(lstm_hyper_params):\n",
    "    num_hyper_params -= 1\n",
    "\n",
    "    json.dumps(phist[\"train_output\"], indent=4)\n",
    "\n",
    "    inference_lstm(\n",
    "        model_path=lstm_fstem + \".keras\",\n",
    "        test_data=pick_random_data(lstm_x_data, lstm_y_data, n=1),\n",
    "    )\n",
    "\n",
    "\n",
    "assert lstm_fstem != \"\", \"fstem is empty\"\n",
    "\n",
    "assert num_hyper_params == 0, f\"{num_hyper_params} != 0\"\n",
    "lstm_fstem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
